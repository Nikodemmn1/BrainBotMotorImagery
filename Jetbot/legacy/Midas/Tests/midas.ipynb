{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import timm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import cv2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_midas\n",
      "load_midas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/olek/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "Using cache found in /home/olek/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('load_midas')\n",
    "PLOT = True\n",
    "\n",
    "class Midas:\n",
    "    def __init__(self):\n",
    "        midas, transform, device = self.load_model_midas()\n",
    "        self.midas = midas\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def load_model_midas(self):\n",
    "        print('load_midas')\n",
    "        # model_t\n",
    "        model_type = \"DPT_BEiT_L_512\" # MiDaS v3.1 - Large (For highest quality - 3.2023)\n",
    "        # model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
    "        # model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
    "        # model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
    "\n",
    "        midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        midas.to(device)\n",
    "        midas.eval()\n",
    "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "        if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "            transform = midas_transforms.dpt_transform\n",
    "        elif model_type == \"DPT_BEiT_L_512\":\n",
    "            transform = midas_transforms.beit512_transform\n",
    "        else:\n",
    "            transform = midas_transforms.small_transform\n",
    "        return midas, transform, device\n",
    "\n",
    "    def predict(self, img):\n",
    "        global IMG_ITERATOR\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # img = img[:,:,:]\n",
    "        #img = img[:, :, ::-1]\n",
    "        input_batch = self.transform(img).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            prediction = self.midas(input_batch)\n",
    "\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.shape[:2],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()\n",
    "        if PLOT:\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(prediction.cpu().numpy())\n",
    "            plt.axis('off')\n",
    "            plt.axhline(10,0,1,color='black')\n",
    "            plt.axhline(230,0,1,color='black')\n",
    "            plt.axvline(22, 0, 1, color='black')\n",
    "            plt.axvline(112, 0, 1, color='black')\n",
    "            plt.axvline(272, 0, 1, color='black')\n",
    "            plt.axvline(362, 0, 1, color='black')\n",
    "            plt.show()\n",
    "        return prediction.cpu().numpy()\n",
    "midas = Midas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MidasInterpreter2:\n",
    "    MIN_SAFE_DISTANCE_MIN = 7250   #20 #7000\n",
    "    MIN_SAFE_DISTANCE =     7500  #25 #7500\n",
    "    MIN_SAFE_DISTANCE_MEAN = 7000 # 21  #6000\n",
    "    RESOLUTION = (384, 384)\n",
    "    GROUP_SIZE = 10\n",
    "    MEAN_PIXEL_COUNT_RATIO = 0.1\n",
    "    MEAN_PIXEL_COUNT = int(RESOLUTION[0] * 0.35 * RESOLUTION[1] * 0.2 * MEAN_PIXEL_COUNT_RATIO)\n",
    "    Y_BOX_POSITION = (10, 230)#330) # split into 10 - 320 - 54\n",
    "    X_BOX_POSITION = (22, 112, 272, 362) # split into 22 - 90 - 160 - 90 - 22\n",
    "\n",
    "    def __init__(self):\n",
    "        self.free_boxes = np.array([False, False, False])\n",
    "\n",
    "    def find_obstacles(self,depth_image):\n",
    "        self.free_boxes = np.array([False, False, False])\n",
    "        left_part = depth_image[self.Y_BOX_POSITION[0]:self.Y_BOX_POSITION[1], self.X_BOX_POSITION[0]:self.X_BOX_POSITION[1]]\n",
    "        mid_part = depth_image[self.Y_BOX_POSITION[0]:self.Y_BOX_POSITION[1], self.X_BOX_POSITION[1]:self.X_BOX_POSITION[2]]\n",
    "        right_part = depth_image[self.Y_BOX_POSITION[0]:self.Y_BOX_POSITION[1], self.X_BOX_POSITION[2]:self.X_BOX_POSITION[3]]\n",
    "\n",
    "        left_depth, left_count = self.look_for_grouping(left_part)\n",
    "        mid_depth, mid_count = self.look_for_grouping(mid_part)\n",
    "        right_depth, right_count = self.look_for_grouping(right_part)\n",
    "\n",
    "        left_average =  self.mean_biggest_values(left_part)\n",
    "        mid_average =  self.mean_biggest_values(mid_part)\n",
    "        right_average =  self.mean_biggest_values(right_part)\n",
    "\n",
    "        left_free = left_depth < self.MIN_SAFE_DISTANCE and left_count < 10 and left_average < self.MIN_SAFE_DISTANCE_MEAN\n",
    "        mid_free = mid_depth < self.MIN_SAFE_DISTANCE and mid_count < 10 and mid_average < self.MIN_SAFE_DISTANCE_MEAN\n",
    "        right_free = right_depth < self.MIN_SAFE_DISTANCE and right_count < 10 and right_average < self.MIN_SAFE_DISTANCE_MEAN\n",
    "\n",
    "        print(f\"Depth prediction:\\n\"\n",
    "              f\"Mean Distances: left={left_average} middle={mid_average} right{right_average}\\n\"\n",
    "              f\"Max Distances: left={left_depth} middle={mid_depth} right{right_depth}\\n\"\n",
    "              f\"Distances Count: left={left_count} middle={mid_count} right{right_count}\\n\"\n",
    "              f\"is_free: left={left_free} middle={mid_free} right{right_free}\")\n",
    "\n",
    "        self.free_boxes = np.array([left_free, mid_free, right_free])\n",
    "        return self.free_boxes\n",
    "\n",
    "    @staticmethod\n",
    "    def look_for_grouping(array):\n",
    "        best_mean = 0.0\n",
    "        count = 0\n",
    "        for x in range(0,array.shape[0],MidasInterpreter2.GROUP_SIZE):\n",
    "            for y in range(0,array.shape[1],MidasInterpreter2.GROUP_SIZE):\n",
    "                grid = array[x:x+MidasInterpreter2.GROUP_SIZE, y:y+MidasInterpreter2.GROUP_SIZE]\n",
    "                mean = grid.mean()\n",
    "                if mean > MidasInterpreter2.MIN_SAFE_DISTANCE_MIN:\n",
    "                    count += 1\n",
    "                if mean > best_mean:\n",
    "                    best_mean = mean\n",
    "        return best_mean, count\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_biggest_values(array):\n",
    "        pixel_count = int(array.shape[0]* array.shape[1] * 0.1)\n",
    "        array = array.flatten()\n",
    "        ind = np.argpartition(array, - pixel_count)[pixel_count:]\n",
    "        return np.average(array[ind])\n",
    "midas_interpreter = MidasInterpreter2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get images\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [16], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(imgs_path)[\u001B[38;5;241m6\u001B[39m:]:\n\u001B[1;32m      6\u001B[0m     frame \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;28mstr\u001B[39m(imgs_path\u001B[38;5;241m+\u001B[39mimg))\n\u001B[0;32m----> 7\u001B[0m     depth_frame \u001B[38;5;241m=\u001B[39m \u001B[43mmidas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     midas_interpreter\u001B[38;5;241m.\u001B[39mfind_obstacles()\n",
      "Cell \u001B[0;32mIn [13], line 35\u001B[0m, in \u001B[0;36mMidas.predict\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mglobal\u001B[39;00m IMG_ITERATOR\n\u001B[0;32m---> 35\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241m.\u001B[39mcvtColor(img, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# img = img[:,:,:]\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;66;03m#img = img[:, :, ::-1]\u001B[39;00m\n\u001B[1;32m     38\u001B[0m     input_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "Cell \u001B[0;32mIn [13], line 35\u001B[0m, in \u001B[0;36mMidas.predict\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mglobal\u001B[39;00m IMG_ITERATOR\n\u001B[0;32m---> 35\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241m.\u001B[39mcvtColor(img, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# img = img[:,:,:]\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;66;03m#img = img[:, :, ::-1]\u001B[39;00m\n\u001B[1;32m     38\u001B[0m     input_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/Templates/pycharm-2022.2.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:880\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    877\u001B[0m             stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m plugin_stop:\n\u001B[0;32m--> 880\u001B[0m     stopped_on_plugin \u001B[38;5;241m=\u001B[39m \u001B[43mplugin_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain_debugger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m stop:\n\u001B[1;32m    882\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_line:\n",
      "File \u001B[0;32m~/Templates/pycharm-2022.2.1/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[0;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[1;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[0;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Templates/pycharm-2022.2.1/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Templates/pycharm-2022.2.1/plugins/python/helpers/pydev/pydevd.py:1169\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finish_debugging_session:\n\u001B[1;32m   1166\u001B[0m     \u001B[38;5;66;03m# before every stop check if matplotlib modules were imported inside script code\u001B[39;00m\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_activate_mpl_if_needed()\n\u001B[0;32m-> 1169\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finish_debugging_session:\n\u001B[1;32m   1170\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmpl_in_use:\n\u001B[1;32m   1171\u001B[0m             \u001B[38;5;66;03m# call input hooks if only matplotlib is in use\u001B[39;00m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('get images')\n",
    "imgs_path = '../../Integration/img/frame/'\n",
    "\n",
    "for img in os.listdir(imgs_path)[6:]:\n",
    "    frame = cv2.imread(str(imgs_path+img))\n",
    "    depth_frame = midas.predict(frame)\n",
    "    midas_interpreter.find_obstacles()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}